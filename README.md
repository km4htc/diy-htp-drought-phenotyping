# DIY High-throughput drought phenotyping

This repository includes information to recreate both the hardware and software used to perform high-throughput drought phenotyping in *Improving rice drought tolerance through host-mediated microbiome selection* (https://doi.org/10.7554/eLife.97015.1).

Traditional drought phenotyping methods are either low-throughput and/or difficult to perform on individual plants without destructive sampling. Spectroscopy can help to overcome these inefficiencies, e.g. by using patterns of light transmission/reflectance to determine chlorophyll content or to derive vegetation indices, allowing samples to be quickly and non-destrucvtively measured. A number of tools already exist to make such measurements, though none that met the particular needs or budget for our project. For example, handheld tools such as a SPAD meter or Photosynq Multispeq are reasonably priced and are quick data collecters, but only capture data from a small portion of individual leaves and cannot be easily used on droughted plants with leaf curling symptoms. Other tools are meant for remote sensing, and employ drones or other vehicles to carry multi/hyperspectral cameras to scan entire fields; while this approach could  easily be scaled down to measure individual plants, the cameras themselves can be prohibitively expensive. 

With the goal of non-destructively phenotyping severely droughted plants in high throughput, I built my own platform using inexpensive and readily available parts. The platform consists of three parts: 1) a pair of DSLR cameras to perform the "multispectral" imaging, 2) a lightbox to control lighting for each image, and 3) a set of scripts to pair data from the two cameras, segment plants from background, then derive vegetation indices to describe a plant's drought status.

### The cameras
The sensors of typical digital cameras are sensitive to light across both the visible (400-700nm) and near-infrared (700-1300) spectra, though IR exclusion filters overlaying the sensor typically restrict the detection of light to just the visible spectrum. Light passing this filter must additionally pass through a Bayer filter--a mosaic of red, green, and blue filters--that further restrict which wavelengths of light reach the sensor and determine the RGB values for a given pixel in an image. As such, the R or Red value for a pixel represents the intensity of red visible light (~550-650nm) reflecting off the surface of the object being photographed; the Green and Blue values indicate the intensity of visible light within ~500-625nm and ~425-525nm, respectively (though there is overlap in the ranges of the red, green, and blue channels, sensitivity typically peaks in the middle of the range).

However, by removing the IR exclusion filter and replacing it with a dual-pass filter that transmits light between 400-600nm as well as between 700-800nm, the  the Red channel can be shifted to detect longer wavelengths of light, including near-infrared, while Blue and Green channels can be left relatively unaffected. Our cameras, then, included one standard DSLR and one modified with the dual-pass filter described above; by pairing the two cameras, we essentially created a multispectral camera with four channels, Red, Green, Blue, and IR. Both standard and modified cameras were Canon DSLRs, models t7 and t1, respectively, and IR exclusion filter removal and replacement was performed by Life Pixel Infrared. Both cameras were set to: shutter speed = 1/40; F-stop = 4.5; ISO = 100; Effect = Neutral; White balance = 7000K; Auto lighting optimizer = OFF. 

The inclusion of IR values allowed us to derive the Normalized Difference Vegetation Index (NDVI), which is a useful measure of overall plant health and stress status. Simply put, NDVI is a measure of the difference of visible light vs infrared light reflected off a plant's surface. While healthy plants strongly absorb (red and blue) visible light via chlorophyll, more of this light will be reflected as plants become stressed and chlorophyll is lost. IR, on the other hand, tends to be strongly reflected regardless of plant stress status. As such, we quantified NDVI as the sum of the median red values from the standard (STD) and modified (MOD) cameras, i.e. visible red and IR, divided by their difference:
```math
NDVI = {MOD_{red} + STD_{red} \over MOD_{red} - STD_{red}} 
```

### The lightbox
A simple lightbox, a 2'x2'x2' cube constructed from MDF, served to control lighting conditions for each image. We painted the interior with "Black 3.0", an ultra light-absorbing black paint from Culture Hustle USA, to minimize reflectance off the surfaces of the box and used a blackout cloth to prevent light leaking through the box door and holes cut to accomodate camera lenses. We affixed alternating strips of full spectrum and 730nm LED lights the entire length of the wall opposite where plants were to be positioned; importantly, while the modified camera could detect the 730nm red light, the standard camera could not. For ease of loading plants, we added a small track made from drawer slides and a corresponding "drawer" that could be loaded with up to 5 five plants at a time (for this project we also built plywood racks to hold five plants with individual pots and water reservoirs; this could then be loaded directly onto the track). Additionally, because downstream object detection of individual plants within images was most easily accomplished if plants did not overlap one another, we built a simple divider, also painted with Black 3.0, to create a boundary between plants.

### Scripts
Lastly, we wrote a series of custom scripts to combine image data for individual plants across standard and modified camera images. These scriptsâ€”-which relied heavily on many tools from PlantCV--can be found elsewhere in the repository, so here I'll just provide a descriptive overview. First, each image was given a unique name that included multiple fields of sample metadata; paired images from the two cameras shared the same name with the exception of a final field noting the camera origin. Within each image, plants were individually segmented from the backround via object detection, then cropped and saved as individual image files (carrying forward metadata from the original filename as well as a unique plant identifier based on its left-right position in the original image). Using masks made during object detection, median RGB values were then extracted from individual plants, and, by matching filenames, data extracted from standard and modified camera images were then paired and used to calculate NDVI using the equation given above.

# Proof of principle
We then performed several experiments to demonstrate that our derived NDVI values are highly predictive of plant chlorophyll and water content, and can be used to track plant drought stress through time. All experiments were performed in a walk-in growth chamber with "Super Dwarf" rice as the plants of interest. 

### Experiment 1 - predicting SPAD values
In this experiment we simply asked how well our image data matched SPAD values--a measure of chlorophyll content--taken with a SPAD meter. In total we measured 119 plants after 30 days of well-watered growth, and took the average of three SPAD measurements (one each from the center of the largest leaves on the largest tillers). We were unable to perform SPAD measurements on droughted plants because it quickly became impossible to clip the SPAD meter on water stressed, curled leaves; consequently, the range of SPAD values assessed was far more narrow than we had hoped for. Interestingly, though NDVI showed a significant linear correlation with SPAD values, several other traits derived from our image data were better predictors of SPAD. These traits, including hue circular mean derived from both cameras individually or median blue-yellow values from the standard camera, more explicitly track the "greenness" of a plant. Whether these traits would remain the top predictors of SPAD across varying levels of drought is unclear, but might be answered using a different plant species whose leaves are larger or more easily manipulated while water stressed.

### Experiment 2 - predicting percent water content
To determine whether NDVI--or other traits derived from image data--could accurately predict plant water content. To do so, we grew rice in well-watered conditions for 30 days, then reduced water reservoirs to 25% original volume for two days before completely emptying reservoirs and withholding water. We  harvested 10 plants each day for 10 days, such that plants harvested later were more drought stressed than those harvested sooner. By measuring the difference between fresh and dry weights (i.e. weight at harvest and weight after a week in a drying oven), we were able to determine plant percent water content at harvest. In contrast to Experiment 1, NDVI was a top predictor of percent water content (PWC) and able to account for up to 96% of variation in PWC data. Several other traits performed similarly well, including red and green chromatic coordinate values from the modified and standard cameras, respectively.

### Experiment 3 - tracking drought in time series
Lastly, we sought to demonstrate how NDVI values for individual plants change throughout drought. As in Experiment 2, rice plants were grown in well-watered conditions for 30 days before withholding then eliminating water completely; however, rather than harvesting a subset of plants each day thereafter, we imaged plants (N=30) daily for 10 days. Not only were we able to show that NDVI declines as drought becomes increasingly severe, but we also found that changes in NDVI were sensitive to--and therefore predictive of--the onset of leaf curling. To demonstrate the latter, we simply fit a smoothed spline to the curve created by an individual plant's NDVI values along 10 days of drought and calculated the point at which the slope was most negative, representing a sharp decrease in NDVI; by cross-referencing these inflection points with the image history of individual plants, we found strong correspondence with which day leaf curling was first visible.
